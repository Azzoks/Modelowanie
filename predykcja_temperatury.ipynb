{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "import numpy as np\n", "import torch\n", "import torch.nn as nn\n", "import matplotlib.pyplot as plt\n", "from sklearn.preprocessing import MinMaxScaler\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Pobranie danych\n", "url = \"https://raw.githubusercontent.com/selva86/datasets/master/daily-min-temperatures.csv\"\n", "df = pd.read_csv(url, parse_dates=['Date'])\n", "\n", "# Wyb\u00f3r tylko temperatury\n", "data = df['Temp'].values.astype(np.float32)\n", "\n", "# Normalizacja danych\n", "scaler = MinMaxScaler()\n", "data_scaled = scaler.fit_transform(data.reshape(-1, 1))\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def create_sequences(data, seq_length=5):\n", "    x, y = [], []\n", "    for i in range(len(data) - seq_length):\n", "        x.append(data[i:i + seq_length])\n", "        y.append(data[i + seq_length])\n", "    return np.array(x), np.array(y)\n", "\n", "x, y = create_sequences(data_scaled, seq_length=5)\n", "\n", "# Podzia\u0142 na zbi\u00f3r treningowy i testowy (80/20)\n", "split = int(len(x) * 0.8)\n", "x_train, x_test = x[:split], x[split:]\n", "y_train, y_test = y[:split], y[split:]\n", "\n", "# Konwersja na tensory PyTorch\n", "x_train = torch.tensor(x_train, dtype=torch.float32)\n", "y_train = torch.tensor(y_train, dtype=torch.float32)\n", "x_test = torch.tensor(x_test, dtype=torch.float32)\n", "y_test = torch.tensor(y_test, dtype=torch.float32)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class LSTM(nn.Module):\n", "    def __init__(self):\n", "        super(LSTM, self).__init__()\n", "        self.lstm = nn.LSTM(1, 32, batch_first=True)\n", "        self.fc = nn.Linear(32, 1)\n", "\n", "    def forward(self, x):\n", "        out, _ = self.lstm(x)\n", "        return self.fc(out[:, -1, :])\n", "\n", "model = LSTM()\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["loss_fn = nn.MSELoss()\n", "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n", "\n", "# Dodajemy wymiar kana\u0142u (LSTM wymaga 3D wej\u015bcia)\n", "x_train = x_train.unsqueeze(-1)\n", "x_test = x_test.unsqueeze(-1)\n", "\n", "# Trening modelu (5 epok \u2013 kr\u00f3tki czas wykonania)\n", "for epoch in range(5):\n", "    for i in range(len(x_train)):\n", "        optimizer.zero_grad()\n", "        y_pred = model(x_train[i:i+1])\n", "        loss = loss_fn(y_pred, y_train[i:i+1])\n", "        loss.backward()\n", "        optimizer.step()\n", "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model.eval()\n", "with torch.no_grad():\n", "    predictions = model(x_test).numpy()\n", "\n", "# Odwracamy skalowanie danych\n", "y_test = scaler.inverse_transform(y_test.numpy().reshape(-1, 1))\n", "predictions = scaler.inverse_transform(predictions)\n", "\n", "# Wykres\n", "plt.plot(y_test[:100], label=\"Rzeczywiste\")\n", "plt.plot(predictions[:100], label=\"Prognozy\")\n", "plt.legend()\n", "plt.show()\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.0"}}, "nbformat": 4, "nbformat_minor": 4}